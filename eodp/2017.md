1. question
    - a) i) http://study.com ii) http://compute.com iii) http://compute.com
    - b)
        ```JSON
        {
            "degree": {
                "subject": {
                    "name" : "COMP20008",
                    "lecture" :"Lec1",
                    "workshop":"Work1"
                }
            }
        }
        ```
    - c) PCA
        - advantages
            - it can redunce the number of features and noise without lossing much informations
            - help clustering and visualisation
        - disadvantages
            - lose information
            - relies on linear assumptions, that is, if their are no linear reltion between data at all, PCA won't help much. ***extra***
    - d) missing data
        - when people doing survey, they not always answer all the questions.
    - e) data wrangling including visualisation,at this stage draw a boxplot may help us to find and remove the outlier.
2. question
    - a)
        - i) the purpose of VAT is to reorder the heatmap and to find the number of k for k-means algorithm (or number of clusters). In this case, put the most similar objects together can help us to visualise.
        - ii) P including the sequence of reordered dissimilarity matrix. Therefore, the new matrix can be created by take the point out of P one by one.
    - b)
        - parallel-coordinates is better
            - parallel-coordinates can tell something about correlations between peers features
        - VAT is better
            - can tell number of clusters
    - c)
        - scenario: people buying stuff online
            - user based methods is to find similar users and recommand the things that user A bought but user B haven't bought yet.
            - item based methods is to find similar items and recommand the similar item based on what the user already bought.

3. question
    - a) no, since pearson correlation needs numerical data but this is categorical data.
    - b) H\(P\)= -(1/2log1/2+1/2log1/2)
    - c) MI(P,A) = H(P ) - H(P|A) = H(P ) + (3/5log3/5+2/5log2/5) NMI(P,A) = MI(P,A) / H(P)
    - d) 6/10
    - e) it is possible, complete flep the class labels.
    - f) unbias way to evaluate how good is classifer
4. question
    - a) row1: 0,sqrt(17),sqrt(32),sqrt(200) row2: sqrt(17),0, 2, 10 row3:sqrt(32),2,0,sqrt(72) row4:1,10,sqrt(72), 0
    - b) question
        - different
            - D' need more calculations, so is slower.
        - similar
            - same centroids
            - same cluster
            - same number of iterations
    - c) question
        - i) It should decrease. as the number of clusters increases, the number of centroids increases, and each cluster become smaller. Therefore, the centroids will closer to each point in that cluster, that is, smaller SSE for each cluster.
        - ii) sa > sb, since clusters in A are more away from their centroids.
    - d) the cost of directly compare is n\*m, but when using block is (u\*m/b), which is much quicker than directly compare but still hold decent accuracy with proper size of b.
5. question
    - a) global sensitive describe how much difference a individual presents or absents affect the result. Higher global sensitive means more noise need to be added to the output.
    - b) 3. Since we have 3 different features, any removal on object will change these three features.
    - c) increase. No effect.
    - d) it is really time consuming to do the preprocessing
    - e) questions
        - i) zip code and Age, since they are the only visiable two and not sensitive information.
        - ii) if know a person live in a place with zip code 305* and are around 30 years old, its POI must be bar.
