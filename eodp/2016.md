1. ignore 1b and 1c
    - a) een(t)+(h)\* or eent+h\* or een\[t\]+\[h\]\*
    - d)
        - i) treat object and feature values as a metric. Using matrix factorization technique to factorize the metric with lowest possible error, which can be calculate by finding the square distance between known values and predicted values. Then fill the missing with the predicted value. **not sure this right or wrong**
        - ii) 1. the data set is huge 2.their may not a specific structue only exist in the instance that has missing value. 3. we want a unbiased distribution of our answer.
2. no ignores
    - a)
        - i)
            - limitation: one limitation is comes from the dataset. By commom sense, students are seldomly more than 10 hours a day. Therefore, the results may not fit these people. Also, Pearson correlation can only tell correlation instead of causality.
            - alternative explanation
                - In a certain range of time, spending more time on study tend to result in a higher score.
                - time spending on study becomes a factores that may positively affact a student score.
        - ii)
            - he made his bins badly
            - wrong calculation
    - b)
        - The meaning of prediction is using the informations that we know to predict unknown informations. If we use the test set as training set, which means using known data to predict know data. The classifer tends to find the data instead of making a prediction. Therefore, the results are likely to be over optimistic.
    - c)
        - H(Parent) = -50/200\*(log50/200) - 150/200\*(log150/200)
        - H(First) = 50/200\*(-log25/50)
        - H(Second) = 150/200\*(-25/150\*log25/150 - 125/150\*(log125/150))
        - gain = H(parent) - H(First) - H(Second)
        - The larger the gain, the better the split
3. no ignore
    - a)
        - |   	| 1         	| 2         	| 3         	|
            |---	|-----------	|-----------	|-----------	|
            | 1 	|     0     	| sqrt(200) 	| sqrt(250) 	|
            | 2 	| sqrt(200) 	| 0         	| sqrt(650) 	|
            | 3 	| sqrt(250) 	| sqrt(650) 	| 0         	|
    - b)
        - before reordering the heatmap, it's normally quite hard for us to identify the number of cluster for the heatmap. After put the similar item closer by using VAT, the pattern(number of clusters) can be found easier.
    - c)
        - No. Since the dataset is too small, no need of reducing features or dimensionality reduction. \\\\
    - d)
        - using parallel coordinates can visualisation the trend of changing for each item at the same time, but VAT can't.
        - VAT is a very stable and systematical way to visualise clusters in heat map. However, the parallel depends on how good the data is ordered.
    - e)
        - the other party may using the known data to guess what is the real value of the hashed value. (not robust to dictionary attack)
4. ignore 4c 4e
    - a)
        - contextual outlier is the concept of data that becomes outlier in the given context.
        - why important
            - to distinguish abnormal data from the dataset based on the given context
    - b)
        - to recommand item to the user that based on similar users comparing to the target user
        - why important
            - give us another choice for collaborative filter
    - d)
        - instead of compare each record in two table, we only compare the blocks when doing table join.
        - why important
            - with proper b size, the join speed up dramaticaclly and still hold a very high accuracy.
5. no ignore
    - a) difference is that one is clustering based method and k-nn is classification based method no need of finding the cluster.
        - cluster based outlier detection
            - advantages: more intuitive and less calculate is needed \\\\
            - disadvantages: number of clusters and the starting point of the centroids may bias the results.
        - KNN
            - advantages: easier to calculate.
            - disadvantages: hard to determine the size of k.
    - b)
        - the normal visit place pair (for example, home and class) may lead to a small set of potential individuals. To solve this, just make sure the k-anonymity, that is, the people is undistinguishable from k others.
        - regular visit places might be found. To solve this, just reduce the frequency of temporal information.
    - c)
        - data wrangling is the process of organising, converting, mapping data from one format to another, which has the most number of steps in data processing.
        - For different data, we are using similar approach to analysis, but before that, how can we tranfer raw data to usable data is a quite challenge and time consuming task.
